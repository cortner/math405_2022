{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MATH 405/607 \n",
    "\n",
    "# Numerical Methods for Differential Equations\n",
    "\n",
    "[[Instructor: Christoph Ortner]](http://www.math.ubc.ca/~ortner/)  [[course page]](https://github.com/cortner/math405_2022)\n",
    "\n",
    "\n",
    "## Solving Nonlinear Systems\n",
    "\n",
    "* Iterative solution of equations\n",
    "* Bisection method\n",
    "* Newton method\n",
    "* Newton method for systems\n",
    "* Fixed point iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "include(\"math405.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Literature\n",
    "\n",
    "* E. Süli and D. Mayer, An Introduction to Numerical Analysis, Ch. 1, 4\n",
    "* https://fncbook.github.io/fnc/nonlineqn/overview.html\n",
    "\n",
    "Further references see end of lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Rootfinding Problem\n",
    "\n",
    "Given an interval $I \\subset \\mathbb{R}$ and $f \\in C(I; \\mathbb{R})$ find $x \\in I$ such that $f(x) = 0$. We call such an $x$ a *root*.\n",
    "\n",
    "In the first part of this lecture we will explore methods to solve this problem and use it also to illustrate the concept of *convergence rates*. In the second part of  the lecture we will extend this to nonlinear systems where $f : \\Omega \\to \\mathbb{R}^N$ with $\\Omega \\subset \\mathbb{R}^N$.\n",
    "\n",
    "In the following in addition to $f$ being continuous we will implicitly assume any regularity required for the argument / calculation that we are performing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "This is taken from [fnc](https://fncbook.github.io/fnc/nonlineqn/demos/roots-bessel.html)\n",
    "\n",
    "In the theory of [vibrations of a circular drum](https://en.wikipedia.org/wiki/Vibrations_of_a_circular_membrane), the displacement of the drumhead can be expressed in terms of pure harmonic modes\n",
    "$$ \n",
    "    J_m(\\omega_{k,m} r) \\cos(m \\theta) \\cos(c \\omega_{k,m} t),\n",
    "$$\n",
    "where the $J_m$ are the *Bessel functions of the first kind*. The *resonant frequencies* $\\omega_{k,m}$ are the positive roots of $J_m$: \n",
    "$$ \n",
    "    J_m(\\omega_{k,m}) = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "using SpecialFunctions \n",
    "plot(x->besselj(3,x), 0, 20, lw=3, label=\"\", grid=:xy, size=(400,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "using NLsolve # or use Roots.jl instead!\n",
    "guess = [6.0, 10.0, 13.0, 16.0, 18.0]\n",
    "R = [ nlsolve(x->besselj(3,x[1]), [x0]).zero[1] for x0 in guess  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot(x->besselj(3,x), 0, 20, lw=3, label=\"\", grid=:xy, size=(500,300))\n",
    "scatter!( guess, besselj.(3, guess), m=:square, ms=7, label = \"guesses\" )\n",
    "scatter!( R, besselj.(3, R), ms=7, label = \"roots\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Most roots (green) are close to the initial guesses (red) but one is quite far away, and in fact because of that we didn't find one of the roots we were looking for. This is a common challenge in the solution of nonlinear equations or systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "?nlsolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So we need to go to the documentational of [NLsolve.jl](https://github.com/JuliaNLSolvers/NLsolve.jl) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... with a bit of digging we can find out that we have used a **Newton-Trust Region Method**. The trust region aspect is a globalisation strategy which we won't study in this class. The core search functionality is provided by the Newton method: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Newton's Method\n",
    "\n",
    "Let $f : I \\to \\mathbb{R}$ and suppose that there is a root $r \\in {\\rm int}(I)$ and that we have a guess $x_n$ for $r$ which is \"close\" to $r$. \n",
    "\n",
    "We can expand $f$ around $x$ \n",
    "\n",
    "$$ \n",
    "f(x+h) = f(x) + f'(x) h + O(h^2)\n",
    "$$\n",
    "\n",
    "If we choose $h$ such that $f(x) + f'(x) h = 0$ then, formally, $f(x+h) = O(h^2)$. That is, if the step $h$ is small then $x+h$ will be close to a root! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This suggests the following *Newton Iteration*: \n",
    "$$ \n",
    "    x_{n+1} := x_n - f(x_n) / f'(x_n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or alternatively we can derive the method geometrically, but this makes it more difficult to understand the convergence.\n",
    "\n",
    "![newtonurl](https://upload.wikimedia.org/wikipedia/commons/e/e0/NewtonIteration_Ani.gif)\n",
    "\n",
    "<!-- \n",
    "Creative Commons Attribution-Share Alike 3.0 (CC BY-SA 3.0)\n",
    "http://creativecommons.org/licenses/by-sa/3.0/\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f = x -> besselj(3, x)\n",
    "df = x -> ForwardDiff.derivative(f, x)\n",
    "x = guess[1]; iter = [x]\n",
    "p = plot(f, 5.5, 7, lw=3, label=L\"J_3\", grid=:xy, size=(800,300), legend=:outertopright)\n",
    "scatter!([x], [f(x)], ms=6, label=\"|f(x_0)| = $(abs(f(x)))\")\n",
    "for n = 2:5.5\n",
    "    x -= f(x) / df(x) ; push!(iter, x)\n",
    "    scatter!([x], [f(x)], ms=6, label=\"|f(x_$n)| = $(abs(f(x)))\")\n",
    "end\n",
    "p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f = x -> besselj(3, x)\n",
    "df = x -> ForwardDiff.derivative(f, x)\n",
    "x = guess[1]\n",
    "p = plot(abs ∘ f, 5.5, 7, lw=2, label=L\"J_3\", grid=:xy, size=(700,300), \n",
    "          legend=:outertopright, yaxis = :log)\n",
    "scatter!([x], [abs(f(x))], ms=6, label=\"|f(x_0)| = $(abs(f(x)))\")\n",
    "for n = 2:5\n",
    "    x -= f(x) / df(x) \n",
    "    scatter!([x], [abs(f(x))], ms=6, label=\"|f(x_$n)| = $(abs(f(x)))\")\n",
    "end\n",
    "p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Look at this more closely: \n",
    "pretty_table( [iter f.(iter) abs.(iter .- iter[end])],  [\"Iterates\", \"f(x)\", \"|x-r|\"],\n",
    "               formatters = (v, i, j) -> (@sprintf(\"%1.20f\", v))  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Newton Method: Local Convergence\n",
    "\n",
    "**Theorem:** Let $f \\in C^2(r - \\delta, r + \\delta)$ for some $\\delta > 0$ and let $f(r) = 0, f'(r) \\neq 0$. Then there exists $\\epsilon > 0$ such that for all $x_0 \\in (r - \\epsilon, r + \\epsilon)$ the Newton iteration converges $x_n \\to r$ as $n \\to \\infty$. Moreover, the convergence is quadratic, i.e., there exists a constant $C$ such that \n",
    "$$ \n",
    "    |x_{n+1} - r| \\leq C |x_n - r|^2\n",
    "$$\n",
    "\n",
    "**Proof:** see tablet/board notes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Newton Method: Global Convergence\n",
    "\n",
    "The example with the roots of the bessel function shows that nonlinear iterations can be unpredictable. Newton's method is a paradigm example of this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f = x -> x^3 - 2x + 2\n",
    "df = x -> 3 * x^2 - 2 \n",
    "\n",
    "function plot_iter(x)\n",
    "    xn = x - f(x) / df(x)\n",
    "    plot(f, -0.5, 1.5, lw=2, label = \"\", size=(600, 400))\n",
    "    plot!([x, xn], [f(x), 0.0], lw=3, ls=:dash, c=:black, label = \"tangent\")\n",
    "    scatter!([x], [f(x)], ms=8, label = \"current\")\n",
    "    return scatter!([xn], [f(xn)], ms=8, label = \"new\"), xn \n",
    "end \n",
    "\n",
    "with_logger(NullLogger()) do    # supress some unhelpful info...\n",
    "    xn = 0.0\n",
    "    anim = @animate for n = 1:4; plt, xn = plot_iter(xn); plt; end\n",
    "    gif(anim, fps=1);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Easter egg:** But Newton's strange behaviour is much more fun to explore in $\\mathbb{C}$. \n",
    "- Consider $z^3 = 1$ in $\\mathbb{C}$. \n",
    "- The solutions are the roots of unity. \n",
    "- For each $z_0 \\in \\mathbb{C}$ apply Newton's method\n",
    "- check to which root it converges. Color accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "using Images\n",
    "function paint(x, y; maxn = 30)\n",
    "    f = z -> z^3 - 1; df = z -> 3*z^2\n",
    "    roots = [ exp(im * 2 * π * n/3) for n in 0:2 ]\n",
    "    n = 0; z = x + im*y\n",
    "    while abs(f(z)) > 1e-10 && n < maxn\n",
    "        z -= f(z) /  df(z)\n",
    "        n += 1\n",
    "    end \n",
    "    i = findmin(abs.(roots .- z))[2]\n",
    "    val = 1 - n/maxn\n",
    "    return RGB(val*(i==1), val*(i==2), val*(i==3))\n",
    "end\n",
    "img = [ paint(x, y) for x in range(-1,1,length=500), y in range(-1,1, length=500) ];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This is called a Newton-type fractal. For more on this see the vast literature on Fractals. For a much better implementation and more examples see [Fatou.jl](https://github.com/chakravala/Fatou.jl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bisection \n",
    "\n",
    "Can we develop a scheme that is less chaotic? Yes, if we use additional information about a potential root:\n",
    "\n",
    "**Lemma [Bolzano]:** Let $f \\in C([a, b]), f(a) f(b) < 0$, then $\\exists$ a root $r \\in (a, b)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f = x -> besselj(3, x)\n",
    "plot(f, 0, 10, lw=3, label=\"\", size=(400, 200))\n",
    "a, b = 5.0, 7.5 \n",
    "scatter!([a,b], [f(a), f(b)], ms=8, label = \"f(a), f(b)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Idea: shrink the interval until $|b-a| < {\\rm TOL}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "function bisection(f, a, b, tol)\n",
    "    while abs(b-a) > tol \n",
    "        c = 0.5 * (a+b)\n",
    "        sign(f(c)) == sign(f(a)) ? a = c : b = c\n",
    "    end             \n",
    "    return 0.5*(a+b)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = bisection(f, a, b, 1e-4)\n",
    "println(\"x = \", x)\n",
    "println(\"f(x) = \", f(x))\n",
    "println(\"error = \", abs(x - R[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's look at the bisection method in action\n",
    "with_logger(NullLogger()) do    # supress some unhelpful info...\n",
    "    a, b = 5.0, 7.5 \n",
    "    plt = plot(f, 0, 10, lw=3, label=\"\", size=(500, 200))\n",
    "    anim = @animate for n = 0:5 \n",
    "        plt = scatter!([a,b], [f(a), f(b)], ms=8, label = \"\", title = \"|a-b| = $(abs(a-b))\")\n",
    "        c = 0.5 * (a+b); sign(f(c)) == sign(f(a)) ? a = c : b = c\n",
    "        plt \n",
    "    end             \n",
    "    gif(anim, fps=1)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\"Clean\" Bisection algorithm:\n",
    "```\n",
    "While abs(b-a) > tol \n",
    "    c = 0.5 * (a+b) \n",
    "    If f(c) == 0\n",
    "       return [a, c]\n",
    "    Elseif sign(f(c)) == sign(f(a))\n",
    "       a = c \n",
    "    Else\n",
    "       b = c\n",
    "    End\n",
    "End\n",
    "return [a, b]    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "**Theorem:** If the input is admissible, then the bisection algorithm terminates after finitely many steps with an interval $[a, b]$ containing a root such that $|b - a| \\leq {\\rm tol}$.\n",
    "\n",
    "**Proof:** After each iteration $|b-a|$ is halved. Therefore after $O(\\log \\tau)$ iterations the algorithm terminates. At each iteration we have $f(a)f(b) \\leq 0$, hence the output contains a root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Convergence of the bisection algorithm\n",
    "\n",
    "Let $[a_n, b_n]$ be the interval at the $n$-th iteration of the bisection algorithm, then \n",
    "$$\n",
    "    |a_n - b_n| = 2^{-n} |a_0 - b_0|\n",
    "$$\n",
    "\n",
    "and in particular \n",
    "$$\n",
    "    |r - 0.5(a_n+b_n)| \\leq 2^{-n-1} |a_0 - b_0|.\n",
    "$$\n",
    "\n",
    "These are instances of *linear convergence*. At each iteration the number of correct digits increases by a constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COST: $\\epsilon(n) \\approx c 2^{-n}$ then $n(\\epsilon) \\approx \\log_2(\\epsilon)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Convergence Rates \n",
    "\n",
    "**Definition:** Let $x_n \\in \\mathbb{F}^N$ and let $d$ be a metric defined on $\\mathbb{F}^N$.\n",
    "1. We say that a sequence $x_n \\to x$ (Q-)linearly with *rate of convergence* $\\mu \\in (0, 1)$ if \n",
    "$$ \n",
    "    \\limsup_{n\\to\\infty} \\frac{d(x_{n+1}, x)}{d(x_n, x)} = \\mu.\n",
    "$$\n",
    "\n",
    "2. We say that $x_n \\to x$ with order $q > 1$ if $x_n \\to x$ and \n",
    "$$ \n",
    "    \\limsup_{n \\to \\infty} \\frac{d(x_{n+1}, x_n)}{d(x_n, x_{n-1})^q} =: M \\in (0, \\infty).\n",
    "$$\n",
    "If $q = 2$ then we call this *quadratic convergence* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. We say that $x_n \\to x$ R-linearly, R-quadratically, etc if there exists a sequence $\\epsilon_n \\to 0$ R-linearly, R-quadratically, etc, such that $d(x_n, x) \\leq \\epsilon_n$. \n",
    "\n",
    "The last definition is to cover cases where the convergence is not very uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nonlinear Systems\n",
    "\n",
    "We now turn towards nonlinear systems. Throughout the remainder of this lecture, we let $D \\subset \\mathbb{R}^N$ be open and simply connected (a *domain*) and $f \\in C(D; \\mathbb{R}^M)$ but as before we assume higher regularity as needed. \n",
    "\n",
    "When we write $f \\in C^p(D; \\mathbb{R}^M)$ we will always mean differentiability in the sense of Frechet. If you are unfamiliar or uncomfortable with this definition, then you can look it up in any multi-variate analysis textbook or lecture notes or if need be on [wikipedia](https://en.wikipedia.org/wiki/Fréchet_derivative). However, for this course a deeper understanding should not should not be required and we can work with a simple and intuitive definition of differentiation, which essentially says that $f$ has a (multi-variate) Taylor expansion: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Definition:** we say that $f$ is differentiable at a point $x_0 \\in D$ if there exists a matrix $\\partial f(x) \\in \\mathbb{R}^{M \\times N}$ such that for all $x \\in D$, \n",
    "\n",
    "$$ \n",
    "    f(x) = f(x_0) + \\partial f(x_0) (x-x_0) + \\epsilon(x_0; x-x_0)\n",
    "$$\n",
    "\n",
    "where $\\| \\epsilon(x_0, x-x_0) \\| = o(\\| x - x_0 \\|)$ (i.e. the remainder may be neglected as $x \\to x_0$. The matrix $\\partial f(x_0)$ is called the Jacobi matrix. (the matrix of partial derivatives)\n",
    "\n",
    "We will normally have at least $f \\in C^2$ and in this case we can replace $\\epsilon$ with $O(\\|x - x_0\\|^2)$. We will assume this from now on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Newton's method for systems \n",
    "\n",
    "That definition also happens to be the derivation of Newton's method: if $f \\in C^2$ then \n",
    "\n",
    "$$\n",
    "    f(x) = f(x_0) + \\partial f(x_0) (x-x_0) + O(\\|x - x_0\\|^2)\n",
    "$$\n",
    "\n",
    "Given an iterate $x_n$ we choose the next iterate $x_{n+1}$ such that \n",
    "\n",
    "$$ \n",
    "    f(x_n) + \\partial f(x_n) (x_{n+1} - x_n) = 0\n",
    "$$\n",
    "\n",
    "This requires the solution of an $N \\times N$ linear system.\n",
    "\n",
    "If $x_n$ is close to a root $r$ then we will  have that $\\| f(x_{n+1})  \\| \\lesssim C \\| x_n - x \\|^2$ and therefore we can expect that $\\| x_{n+1} - r \\| \\leq C \\| x_n - r \\|^2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Newton iteration: \n",
    "$$ \n",
    "    x_{n+1} = x_n - \\partial f(x_n)^{-1} f(x_n)\n",
    "$$\n",
    "\n",
    "**Theorem:** Suppose that $f \\in C^2(D; \\mathbb{R}^N)$, $r \\in D$, $f(r) = 0$ and $\\partial f(r)$ is non-singular, then there exists $\\epsilon > 0$ such that for $\\|x_0 - r\\| \\leq \\epsilon$ the Newton-iteration is well-defined and $x_n \\to r$ quadratically.\n",
    "\n",
    "**Proof:** see board/tablet/recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "function newton(f, x0, tol, df = x -> ForwardDiff.jacobian(f, x))\n",
    "    while norm(f(x)) > tol \n",
    "        x = x - df(x) \\ f(x)\n",
    "    end \n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "function newtonhist(f, x0, niter, df = x -> ForwardDiff.jacobian(f, x))\n",
    "    hist = [x0]; x = x0 \n",
    "    for n = 1:niter\n",
    "        x -= df(x) \\ f(x) \n",
    "        push!(hist, x)\n",
    "    end\n",
    "    return hist \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example:** two simultaneous equations defining the intersection of 2 ellipsi\n",
    "$$\n",
    "x_1^2 + x_2^2 = 1, \\qquad \\qquad \n",
    "    5 x_1^2 + 21 x_2^2 = 9\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tt = range(0, 2*pi, length = 300)\n",
    "function _plot_sys()\n",
    "    plot( cos.(tt), sin.(tt), lw=3, label = \"Eqn 1\", size = (350,250), legend=:outertopright)\n",
    "    plot!( 3/√5 * cos.(tt), 3/√21 * sin.(tt), lw=3, label = \"Eqn 2\")\n",
    "end \n",
    "_plot_sys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f = x -> [ x[1]^2+x[2]^2 - 1, 5*x[1]^2+21*x[2]^2 - 9 ]\n",
    "X = newtonhist(f, [0.3,0.3], 7)\n",
    "_plot_sys()\n",
    "scatter!( [x[1] for x in X], [x[2] for x in X], ms=6, label = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pretty_table( [[x[1] for x in X] [x[2] for x in X] norm.(f.(X))], \n",
    "               [\"x1\", \"x2\", \"|f(x)|\"], \n",
    "              formatters = (v, i, j) -> (@sprintf(\"%1.20f\", v))  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Main message is : the theory and practise of Newton's method for system is essentally the same as for scalar problems.\n",
    "\n",
    "- for non-singular systems the local convergence is very rapid (quadratic) \n",
    "- global convergence is not guaranteed, and difficult to predict.\n",
    "- For many practical problems it is difficult to get the Jacobi matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixed Point Iterations\n",
    "\n",
    "For many nonlinear problems there will be problem-specific iteration strategies that might include some ideas of Newton's method (or not...) Most of these iterations are best thought of as fixed-point iterations: \n",
    "- construct a *fixed point map* $T : \\mathbb{R}^N \\to \\mathbb{R}^N$, or $T : D \\to D$ such that $f(x) = 0$ iff. $T(x) = x$. \n",
    "- iterate $x_{n+1} = T(x_n)$.\n",
    "\n",
    "We will treat this topic only theoretically and by means of a few examples.\n",
    "\n",
    "<!-- Trivial construction: $T(x) = x - \\alpha f(x)$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example 1:** \n",
    "$$ \n",
    "    T(x) = x + \\alpha f(x)\n",
    "$$\n",
    "then clearly $f(x) = 0$ iff $T(x) = x$. This can be thought of as a discretisation of the ODE $\\dot{x} = f(x)$. If $r$ is a *stable* equilibrium then for sufficiently small $\\alpha$ and $x_0$ sufficiently close to  $r$ we have $x_n \\to r$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example 2:** The Newton iteration \n",
    "\n",
    "$$\n",
    "  x_{n+1} = x_n - \\partial f(x_n)^{-1} f(x_n)\n",
    "$$\n",
    "\n",
    "is a fixed point iteration $x_{n+1} = T(x_n)$ with operator\n",
    "\n",
    "$$ \n",
    "T(x) = x - \\partial f(x)^{-1} f(x)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example 3:** Many nonlinear PDEs can be rewritten in a fixed-point form, e.g., consider \n",
    "$$\n",
    "   \\nabla \\cdot \\big( a(x, u(x)) \\nabla u(x) \\big) = b(x, u(x))\n",
    "$$\n",
    "\n",
    "Then we can define a fixed point operator by : to get the mapping $u \\mapsto Tu$ solve the linear equation\n",
    "$$ \n",
    "   \\nabla \\cdot \\big( a(x, u(x)) \\nabla Tu(x) \\big) =  b(x, u(x))\n",
    "$$\n",
    "for $Tu$. $T(u)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fixed Point Theorems\n",
    "\n",
    "There are numerous results that will establish the existence of fixed points, the convergence of fixed point iterations, or the acceleration of fixed point iterations. Here we mention just two elementary results of this kind: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Theorem:** (Brouwer's fixed point theorem) Let $D \\subset \\mathbb{R}^N$ be non-empty, closed, bounded, and convex and $T : D \\to D$  continuous, then there exists $x_* \\in D$ s.t. $T(x_*) = x_*$.\n",
    "\n",
    "This is quite abstract and does not say much about iterations and practical computations. One usually aims for stronger results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Theorem:** (Contraction mapping theorem, Banach's fixed point theorem) \n",
    "Suppose that $D \\subset \\mathbb{R}^N$ is closed, $T : D \\to D$ is a contraction, i.e., $\\|T(x) - T(y)\\| \\leq \\eta \\|x - y \\|$ where $\\eta \\in (0, 1)$, then there exists a unique fixed point in $D$ and the iteration $x_{n+1} = T(x_n)$ converges linearly with rate $\\eta$.\n",
    "\n",
    "**Proof:** see any introductory functional analysis textbook, or Mayers & Süli, §4.2.\n",
    "\n",
    "<br> \n",
    "\n",
    "In fact, this is one way to prove convergence of Newton's method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Further Topics \n",
    "\n",
    "* Damped Newton method: consider steps $x_{n+1} = x_n - \\alpha_n \\partial f(x_n)^{-1} f(x_n)$ and choose $\\alpha_n$ to control the convergence, see `NLsolve.jl`\n",
    "* Line search methods: means to determine step-lengths $\\alpha_n$, see `NLsolve.jl`\n",
    "* Trust region methods: alternative globalisation strategy, see `NLsolve.jl`\n",
    "* Quasi-Newton methods: Replace $\\partial f(x_n)^{-1}$ with a low-rank approximation which is estimated from past iterates \n",
    "* Jacobian-free methods: Replace $\\partial f(x_n) \\cdot u$ with $(f(x_n + \\delta u) - f(x_n))/\\delta$ and use iterative linear solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Further reading \n",
    "* C. T. Kelley, Iterative Methods for Linear and Nonlinear Equations [pdf](https://archive.siam.org/books/textbooks/fr16_book.pdf)\n",
    "* J. M. Ortega and W. C. Rheinboldt. Iterative Solution of Nonlinear Equations in Several Variables. Elsevier, 2014.\n",
    "* C. T. Kelley, Solving Nonlinear Equations with Newton's Method\n",
    "* P. Deuflhard, Newton Methods for Nonlinear Problems - Affine Invariance and Adaptive Algorithms\n",
    "* J. Nocedal and S. Wright, Numerical Optimization (I will try to fit in a class on nonlinear optimization later in the course)\n",
    "\n",
    "In Julia see \n",
    "* [NLSolve.jl](https://github.com/JuliaNLSolvers/NLsolve.jl) : nonlinear systems\n",
    "* [Roots.jl](https://github.com/JuliaMath/Roots.jl) : solving scalar nonlinear equations\n",
    "* [SIAMFANLEquations.jl](https://github.com/ctkelley/SIAMFANLEquations.jl) : Kelley's codes"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
