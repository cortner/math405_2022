{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MATH 405 - Numerical Methods for Differential Equations\n",
    "\n",
    "# MATH 607 - Topics in Numerical Analysis\n",
    "\n",
    "[[Instructor: Christoph Ortner]](http://www.math.ubc.ca/~ortner/)   [[course page]](https://github.com/cortner/math405_2022)\n",
    "\n",
    "\n",
    "\n",
    "# L05 - Floating Point Arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# L05 - Floating Point Arithmetic\n",
    "\n",
    "The content of this lecture is not Julia specific but applies to all languages! The topic may seem dry and tedious, and as a matter of fact we want to minimize our engagement with it. We can afford to do so because of people like Kahan who have spent their lives ensuring we have robust floating point arithmetic available now. But it is important that we at least  have a basic understanding.\n",
    "\n",
    "#### Further Reading \n",
    "\n",
    "* Michael Overton, Numerical Computing with IEEE Floating Point Arithmetic, SIAM \n",
    "* Nicholas Higham, Accuracy and Stability of Numerical Algorithms, SIAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Integers\n",
    "\n",
    "$$ \n",
    "    \\dots, -3, -2, -1, 0, 1, 2, 3, \\dots \n",
    "$$\n",
    "\n",
    "Integers are, by default, of type `Int = Int64`. They are represented as 64 bit (8 byte) in memory (base - 2). E.g. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3, typeof(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show bitstring(3); \n",
    "@show bitstring(253780027);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Integer arithmetic is exact "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 + 3 ==  5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... except when we overflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show 2^62, bitstring(2^63-1);\n",
    "@show 2^63, bitstring(2^63);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fractions \n",
    "\n",
    "This is an interesting subject, especially in Julia, but we don't have time to cover it, so I will skip it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3 // 5, typeof(3 // 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3//5 * 5//3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The basic problem with fraction is that if we perform many operations then they get longer and longer to store. A few or even 10s of operations might be ok, but we will deal with 1000s to millions of operations. We either get overflow or need complicated and compuational expensive schemes to store those big fractions. (see `?big`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "3//5 * 13/17 * 29/57 * 11/56 * 37 / 87"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Floating Point Numbers\n",
    "\n",
    "General real numbers cannot be represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show x = 5/7\n",
    "@show y = (x / 30) * 210;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show y == 5    # y should be 5\n",
    "@show y ≈ 5     # but it is only approximately equal to 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# or even simpler \n",
    "@show (0.1 + 0.2) + 0.3\n",
    "@show 0.1 + (0.2 + 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The advantage of this is that we can use numbers that aren't fractions, e.g., "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sqrt(2)\n",
    "@show x^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# To help us compare floating point numbers Julia gives us the ≈ operator: \n",
    "@show x^2 ≈ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 32 bit Floating point number \n",
    "\n",
    "$$ \n",
    "    x = s \\cdot m \\times 2^e\n",
    "$$\n",
    "where $s \\in \\{1, -1\\}$ is the sign, $m > 0$ the mantissa (fraction) and $e > 0$ the exponent.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/d/d2/Float_example.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "E.g., \n",
    "$$ \n",
    "    12.2792 = 0.122792 \\times 10^2\n",
    "$$\n",
    "(except of course in base 2 rather than base 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitstring(Float32(12.2792))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Floating point operations \n",
    "\n",
    "Crudely, we can think of a floating point operation ${\\rm fl}(a \\odot b)$ as follows: \n",
    "1. exact operation $c = a \\odot b$ \n",
    "2. rounding $d = {\\rm round}(d)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show a = Float32(sqrt(2))\n",
    "@show b = Float32(exp(1))\n",
    "# \"exact\" operation by going to more accurate arithmetic\n",
    "@show c = Float64(a) + Float64(b)\n",
    "@show d = a + b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There is much more we should say about floating point arithmetic, but we will keep it to these basics, just a few more comments: \n",
    "* Standard accuracy is 64 bit, but sometimes 32bit are use for speed.\n",
    "* Floating point numbers have a range, i.e. one can underflow or overflow\n",
    "* Floating point numbers are roughly logarithmically distributed in that range, i.e. relative floating point accuracy is the same at all scales (within the range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show floatmin(Float64), floatmax(Float64)   # the range\n",
    "@show floatmin(Float32), floatmax(Float32)   # the range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Standard Model of Floating Point Arithmetic\n",
    "\n",
    "Let $\\odot$ denote one of the four arithmetic operations, $+, -, \\times, \\div$. Then in the standard model for floating point arithmetic is to assume that \n",
    "$$ \n",
    "    {\\rm fl}[ a \\odot b ] = (a \\odot b) (1+\\eta)\n",
    "$$\n",
    "where $|\\eta| \\leq \\epsilon$ and $\\epsilon > 0$ is *machine precision*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show eps(Float64); \n",
    "@show eps(Float32);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: finite difference formulas\n",
    "\n",
    "In exact Arithmetic: \n",
    "$$ \n",
    "    \\frac{f(x+h) - f(x)}{h} = f'(x) + O(h)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In floating point arithmetic: \n",
    "$$\\begin{aligned}\n",
    "    {\\rm fl}\\Big[ \\frac{f(x+h) - f(x)}{h}\\Big]\n",
    "    &=  \n",
    "    \\frac{ {\\rm fl}[f(x+h) - f(x)] }{h} (1+\\eta_1) \\\\ \n",
    "%     &= \n",
    "%     \\frac{(f(x+h) - f(x))(1+\\eta_2)}{h} (1+\\eta_1) \\\\ \n",
    "    &= \n",
    "    \\frac{{\\rm fl}[f(x+h)] - {\\rm fl}[f(x)]}{h} \\frac{1+\\eta_2}{1+\\eta_1} \\\\             \n",
    "    &= \n",
    "    \\frac{f(x+h)(1+\\eta_3) - f(x)(1+\\eta_4)}{h} \\frac{1+\\eta_2}{1+\\eta_1} \\\\   \n",
    "    &= \n",
    "    \\frac{f(x+h) - f(x)}{h}  \\frac{1+\\eta_2}{1+\\eta_1} \n",
    "    +  \\frac{f(x+h) \\eta_3 - f(x) \\eta_4}{h} \\frac{1+\\eta_2}{1+\\eta_1} \\\\   \n",
    "    &=\n",
    "    \\frac{f(x+h) - f(x)}{h} (1+O(\\epsilon)) + O(\\epsilon/h)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Summary: \n",
    "$$ \n",
    "{\\rm fl}\\Big[ \\frac{f(x+h) - f(x)}{h}\\Big] = f'(x) + O(h) + O(\\epsilon/h)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "using Plots \n",
    "hh = 0.5.^(4:45)\n",
    "f = x -> cos(x); df = x -> - sin(x); x0 = pi/4  # what happens if we change pi/4 -> pi/2 ??\n",
    "err_ex = [ abs((f(big(x0+h)) - f(big(x0)))/big(h) - df(x0)) for h in hh ]\n",
    "err_fl = [ abs((f(x0+h) - f(x0))/h - df(x0)) for h in hh ]\n",
    "plot(hh, err_ex, xaxis = :log, xlabel = \"h\", yaxis = :log, ylabel = \"err\", lw = 2, label = \"exact\", size = (700, 300), ylims = [1e-15, 1e-2], legend = :outertopright)\n",
    "plot!(hh, err_fl, label = \"Float64\", lw = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Things can get arbitrarily bad ... \n",
    "\n",
    "The following incredible example is due to [Stefan Karpinski](https://discourse.julialang.org/t/array-ordering-and-naive-summation/1929).  (Julia co-developer, but the example has nothing to do with Julia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sumsto function always returns the same 2046 floating-point numbers but \n",
    "# returns them in a different order based on x\n",
    "function sumsto(x::Float64)\n",
    "    0 <= x < exp2(970) || throw(ArgumentError(\"sum must be in [0,2^970)\"))\n",
    "    n, p₀ = Base.decompose(x) # integers such that `n*exp2(p₀) == x`\n",
    "    [floatmax(); [exp2(p) for p in -1074:969 if iseven(n >> (p-p₀))]\n",
    "    -floatmax(); [exp2(p) for p in -1074:969 if isodd(n >> (p-p₀))]]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = sumsto(3.142592654)    #  pi\n",
    "X2 = sumsto(2.99792458e8)   #  speed of light\n",
    "@show sort(X1) == sort(X2)  #  the two arrays X1, x2 contain the same numbers!\n",
    "@show sum(x for x in X1)    #  but sum to entirely different values \n",
    "@show sum(x for x in X2);   #    (i.e. order of summation matters!)\n",
    "@show sum(X1), sum(X2)      # Note that a plain `sum` is numerically more robust! (cf compensated summation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " .... which is why numerical stability is so important!  If  you think this was a fun example and would like to understand it then look up Kahan summation or compensated summation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Numerical Stability in Linear Algebra \n",
    "\n",
    "Solving linear systems \n",
    "$$\n",
    "Ax = b\n",
    "$$ \n",
    "is one of the most important everyday tasks in numerical computations. It is therefore paramount that we use numerically stable linear solvers!\n",
    "\n",
    "We already briefly touched on conditioning which is related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a simple well-conditioned matrix. the resulting linear system is therefore also well-conditioned, i.e. it should be possible to construct the solution in a numerically stable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ϵ = 1e-15\n",
    "A = [ ϵ    1.0\n",
    "      1.0  1.0 ]\n",
    "cond(A) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# create a manufactured solution\n",
    "x = rand(2)\n",
    "b = A * x;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But if we solve it with an unstable algorithm ... (note that in Julia one has to manually force the usage of an unstable algorithm. The defaults are very good.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "x2 = lu(A, NoPivot()) \\ b \n",
    "norm(x2 - x, Inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And now with a stable algorithm ... (technically, this uses only partial pivoting, but it appears to suffice almost always!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = lu(A) \\ b    # same as A \\ b\n",
    "norm(x - x1, Inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Another linear system - this time it is ill-conditioned to begin with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ϵ = 1e-14\n",
    "A = [2.0 2.0 - ϵ\n",
    "     1.0 1.0]\n",
    "cond(A) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand(2); b = A * x \n",
    "x1 = lu(A, NoPivot()) \\ b \n",
    "norm(x1 - x, Inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Pivoting has no effect this time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = lu(A) \\ b \n",
    "norm(x2 - x, Inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# but notice that the residual is still small!\n",
    "norm(A * x1 - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Backward Stability \n",
    "\n",
    "LU Factorisation with Complete Pivoting is Backward Stable. With Partial Pivoting it is stable most of the time. What do we mean by that? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Meta-Theorem:** Let $PA = LU$ be the (pivoted) LU factorisation computed in exact arithmetic and let $\\tilde{L}, \\tilde{U}$ be the LU factors computed in floating point arithmetic. Then there exists a matrix $\\tilde{A}$ such that $P\\tilde{A} = \\tilde{L} \\tilde{U}$ and moreover \n",
    "$$\n",
    "    \\frac{\\| A - \\tilde{A} \\|}{\\|A\\|} \\leq C \\epsilon.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Loosely speaking (not that loosely actually - what is missing?), we can say there exists a perturbed problem \n",
    "$$\n",
    "  \\tilde{A} \\tilde{x} = b \n",
    "$$\n",
    "which we solve when working with floating point arithmetic. In particular, if $A$ or equivalently $\\tilde{A}$ are well-conditioned, then we can use that to demonstrate that the error in the solution will be small as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\begin{aligned}\n",
    "   \\tilde{A}\\tilde{x} &= b, \\qquad A x = b \\\\ \n",
    "    A (x - \\tilde{x}) &= Ax - A \\tilde{x} \\\\ \n",
    "      &= \\tilde{A} \\tilde{x} - A \\tilde{x} \\\\ \n",
    "      &= (\\tilde{A} - A) \\tilde{x}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can deduce \n",
    "$$\\begin{aligned}\n",
    "    \\|x - \\tilde{x} \\| &\\leq \\|A^{-1}\\| \\| \\tilde{A} - A \\| \\| \\tilde{x} \\| \\\\ \n",
    "     \\frac{\\|x - \\tilde{x} \\|}{\\|\\tilde{x}\\|} &\\leq \\kappa(A) \\frac{\\| \\tilde{A} - A \\|}{\\|A\\|} \\leq C \\epsilon\n",
    "\\end{aligned}$$\n",
    "If the linear system is well-conditioned, then the relative error will be on the order of machine precision. \n",
    "\n",
    "Many further refinements to this analysis are of course possible; this is only intended to give you the main idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    " \n",
    "### Conclusions\n",
    "\n",
    " * Well-conditioned problems still need numerically stable algorithms! \n",
    " * If the problem is ill-conditioned to begin with then no numerics can save us. **BUT** sometimes there is a more hidden way to understand conditioning. It is not always obvious. \n",
    " * More often than not we need not worry about numerical stability, generations of Numerical Analysts have already done this for us!\n",
    " * But be aware that the problem does exist and where to start looking in case you run into it!\n",
    " * Numerical stability is not always intuitive. E.g. how can naive summation be a problem? Othertimes an expression that looks incredibly unstable turns out to be perfectly fine, e.g. see A2 - barycentric formula.\n",
    " * `Float64` is the de-factor standard in numerical computations. \n",
    " * When you really need more accuracy and are willing to sacrifice performance consider using \n",
    "     - `Float128` which is implemented in [`QuadMath.jl`](https://github.com/JuliaMath/Quadmath.jl)\n",
    "     - or `BigFloat` which is built-in and can be accessed via `big`; cf. `?big` for more detail.\n",
    "     - Use these with great care!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Reading \n",
    "\n",
    "* Michael Overton, Numerical Computing with IEEE Floating Point Arithmetic, SIAM \n",
    "* Nicholas Higham, Accuracy and Stability of Numerical Algorithms, SIAM"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
